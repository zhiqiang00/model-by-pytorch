{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "mount_file_id": "1OmNzIyv1jrOClEOqPZYqnxSE5L9MtXj9",
      "authorship_tag": "ABX9TyPHl26AdkRWvuCv8tVQHTzU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhiqiang00/model-by-pytorch/blob/main/Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_W2jYckek0U"
      },
      "source": [
        "# PyTorch 实现 Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShDME-1rxgT4",
        "outputId": "f4e6b81b-72a9-41ca-deb4-f0ce705f7561"
      },
      "source": [
        "!pip install sklearn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9H4ixCYZQ-m"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as tud\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "C = 3\n",
        "K = 15\n",
        "epochs = 2\n",
        "MAX_VOCAB_SIZE = 10000\n",
        "EMBEDDING_SIZE = 100\n",
        "batch_size = 32\n",
        "lr = 0.2\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkmNYKEbv8_c"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o422OVkEwT28"
      },
      "source": [
        "with open('/content/drive/MyDrive/MLLearning/data/text8/text8.train.txt') as f:\n",
        "  text = f.read()\n",
        "\n",
        "text = text.lower().split()\n",
        "vocab_dict = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1))\n",
        "vocab_dict['<UNK>'] = len(text) - np.sum(list(vocab_dict.values()))\n",
        "\n",
        "word2idx = {word:i for i, word in enumerate(vocab_dict.keys())}\n",
        "idx2word = {i:word for i, word in enumerate(vocab_dict.keys())}\n",
        "word_counts = np.array([count for count in vocab_dict.values()], dtype=np.float32)\n",
        "word_freqs = word_counts / np.sum(word_counts)\n",
        "word_freqs = word_freqs ** (3. / 4.)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_MeSKcu84gV"
      },
      "source": [
        "# 实现 DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rcC_fMt2eqS"
      },
      "source": [
        "class WordEmbeddingDataset(tud.Dataset):\n",
        "  def __init__(self, text, word2idx, word_freqs):\n",
        "    '''text: a list of words, all text from the training dataset\n",
        "      word2idx: the dictionary from word to index\n",
        "      word_freqs: the frequency of each word\n",
        "    '''\n",
        "    super(WordEmbeddingDataset, self).__init__() # #通过父类初始化模型，然后重写两个方法\n",
        "    self.text_encoded = [word2idx.get(word, word2idx['<UNK>']) for word in text]\n",
        "    self.text_encoded = torch.LongTensor(self.text_encoded)\n",
        "    self.word2idx = word2idx\n",
        "    self.word_freqs = torch.Tensor(word_freqs)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text_encoded) #返回所有单词的总数 即item的总数\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    '''这个function返回以下数据用于训练\n",
        "      - 中心词\n",
        "      - 这个单词附近的positive word\n",
        "      - 随机采样的K个单词作为negative word\n",
        "    '''\n",
        "    center_words = self.text_encoded[idx] #取中心词\n",
        "    pos_indices = list(range(idx - C, idx)) + list(range(idx + 1, idx + C + 1))\n",
        "    pos_indices = [i % len(self.text_encoded) for i in pos_indices] # 取余数 避免越界\n",
        "    pos_words = self.text_encoded[pos_indices]\n",
        "    neg_words = torch.multinomial(self.word_freqs, K*pos_words.shape[0], True)\n",
        "\n",
        "      # while循环证不包含背景词,如果取交集len大于零重新采样\n",
        "    while len(set(pos_words.numpy().tolist()) & set(neg_words.numpy().tolist())) > 0:\n",
        "      neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], True)\n",
        "\n",
        "    return center_words, pos_words, neg_words\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g7LSLrMAb_d"
      },
      "source": [
        "dataset = WordEmbeddingDataset(text, word2idx, word_freqs)\n",
        "dataloader = tud.DataLoader(dataset, batch_size, shuffle=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv2agGXYMMDi"
      },
      "source": [
        "# 定义Pytorch模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKZPaxrmFr9E"
      },
      "source": [
        "class EmbeddingModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size):\n",
        "    super(EmbeddingModel, self).__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed_size = embed_size\n",
        "\n",
        "    self.in_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
        "    self.out_embed = nn.Embedding(self.vocab_size, self.embed_size) \n",
        "  \n",
        "  def forward(self, input_labels, pos_labels, neg_labels):\n",
        "    '''input_labels: center words, [batch_size]\n",
        "      pos_labels: positive words, [batch_size, (window_size * 2)]\n",
        "      neg_labels：negative words, [batch_size, (window_size * 2 * K)]\n",
        "            \n",
        "      return: loss, [batch_size]\n",
        "    '''\n",
        "    input_embedding = self.in_embed(input_labels)\n",
        "    pos_embedding = self.out_embed(pos_labels)\n",
        "    neg_embedding = self.out_embed(neg_labels)\n",
        "\n",
        "    input_embedding = input_embedding.unsqueeze(2)\n",
        "    pos_dot = torch.bmm(pos_embedding, input_embedding) #32*6*1 = 32*6*100 X 32*100*1\n",
        "    pos_dot = pos_dot.squeeze(2)\n",
        "\n",
        "    neg_dot = torch.bmm(neg_embedding, -input_embedding) # [batch_size, (window * 2 * K), 1]\n",
        "    neg_dot = neg_dot.squeeze(2) # batch_size, (window * 2 * K)]\n",
        "\n",
        "    log_pos = F.logsigmoid(pos_dot).sum(1)\n",
        "\n",
        "    log_neg = F.logsigmoid(neg_dot).sum(1) \n",
        "\n",
        "    loss = log_pos + log_neg #logsigmoid的结果全部为负数，所以loss取负数\n",
        "\n",
        "    return -loss\n",
        "  def input_embedding(self):\n",
        "    return self.in_embed.weight.detach.numpy()\n",
        "\n",
        "model = EmbeddingModel(MAX_VOCAB_SIZE, EMBEDDING_SIZE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTQsw2VzSOxR"
      },
      "source": [
        "# 模型训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlC_BnqTgOmn"
      },
      "source": [
        "for e in range(1):\n",
        "  for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
        "    input_labels = input_labels.long()\n",
        "    pos_labels = pos_labels.long()\n",
        "    neg_labels = neg_labels.long()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = model(input_labels, pos_labels, neg_labels).mean()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if i%100 == 0:\n",
        "      print('epoch', e, 'iteration', i, loss.item())\n",
        "\n",
        "embedding_weights = model.input_embedding()\n",
        "torch.save(model.load_state_dict(), \"embedding-{}.th\".format(EMBEDDING_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW46V4jLZVMW"
      },
      "source": [
        "# 词向量应用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLjTu0upzlnI"
      },
      "source": [
        "def find_nearest(word):\n",
        "  index = word2idx[word]\n",
        "  embedding = embedding_weights[index]\n",
        "  cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
        "  return [idx2word[i] for i in cos_dis.argsort()[:10]]\n",
        "\n",
        "for word in [\"two\", \"america\", \"computer\"]:\n",
        "  print(word, find_nearest(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DSlAUfdZTsQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}